# Qwen-7B-Chat
| ğŸ¤—[Huagging Face](https://huggingface.co/Qwen/Qwen-7B-Chat) | ğŸ¤– [ModelScope](https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary) | [GitHub](https://github.com/QwenLM/Qwen-7B) | [Demo](https://modelscope.cn/studios/qwen/Qwen-7B-Chat-Demo/summary) | [Report](https://github.com/QwenLM/Qwen-7B/blob/main/tech_memo.md) |

SuperCLUE-Open é‡‡ç”¨ä»¥ä¸‹ä»£ç è¿›è¡Œæµ‹è¯„ï¼š

```
import os 
os.environ['TRANSFORMERS_CACHE'] = './weight'
from fastapi import FastAPI, Request
import uvicorn
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from transformers.generation import GenerationConfig

app = FastAPI()

@app.post('/api/api_server_03')
async def chat_completion(request: Request):
    data = await request.json()
    prompt = data['prompt']
    print(prompt)
    history = data['history']
    if len(history) == 0:
        print('First time to chat...')
        history = None
    response, history = model.chat(tokenizer, prompt, history=history)
    return {'response': response, 'history': history}


if __name__ == '__main__':
    tokenizer = AutoTokenizer.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True)
    ## æ‰“å¼€bf16ç²¾åº¦ï¼ŒA100ã€H100ã€RTX3060ã€RTX3070ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
    # model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, bf16=True).eval()
    ## æ‰“å¼€fp16ç²¾åº¦ï¼ŒV100ã€P100ã€T4ç­‰æ˜¾å¡å»ºè®®å¯ç”¨ä»¥èŠ‚çœæ˜¾å­˜
    # model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True, fp16=True).eval()
    # é»˜è®¤ä½¿ç”¨fp32ç²¾åº¦
    model = AutoModelForCausalLM.from_pretrained("Qwen/Qwen-7B-Chat", device_map="auto", trust_remote_code=True).eval()
    model.generation_config = GenerationConfig.from_pretrained("Qwen/Qwen-7B-Chat", trust_remote_code=True) # å¯æŒ‡å®šä¸åŒçš„ç”Ÿæˆé•¿åº¦ã€top_pç­‰ç›¸å…³è¶…å‚
    print(f'temperature: {model.generation_config.temperature}')
    print(f'max_new_tokens: {model.generation_config.max_new_tokens}')
    # temperature: 1.0
    # max_new_tokens: 512
    uvicorn.run(app, host='0.0.0.0', port=9103)

'''
æ‰§è¡Œå‘½ä»¤ï¼š
conda activate llama
CUDA_VISIBLE_DEVICES=0 nohup python -u api_qwen_7b_chat.py > qwen.log 2>&1 &
'''
```

ä»¥ä¸Šä»£ç ä¿®æ”¹è‡ªï¼šhttps://github.com/QwenLM/Qwen-7B#-transformers